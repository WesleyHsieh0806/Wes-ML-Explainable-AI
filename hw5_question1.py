# -*- coding: utf-8 -*-
"""hw5_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FbuTOevZTUO3IEVJLwSfwCdGnrBf3Qwv

# CNN Explaination

[作業說明投影片](https://docs.google.com/presentation/d/1VClvgyilAvohextY0tM3gD7YemXGSUrzLV0E8RjDnMU/edit?usp=sharing)

如果有自己的計算資源，不想使用 colab 的話，可以參考純 [Python script](https://github.com/leo19941227/MLHW4-Explainable) 的版本

若有任何問題，歡迎來信至助教信箱： ntu-ml-2020spring-ta@googlegroups.com

# Environment settings
"""


"""## Start our python script"""


"""## Argument parsing"""
# ckpt代表checkpoint 這種檔案用來儲存model參數的值
# 也有其他種如meta是存整個model包含他的model 結構圖
import random
import os
import sys
import argparse
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from skimage.segmentation import slic
from lime import lime_image
from pdb import set_trace
import cv2
from torch.utils.data import DataLoader
torch.manual_seed(0)
args = {
    'ckptpath': './CNN_model.h5',
    'dataset_dir': sys.argv[1]
}
# 一般會用ArgumentParser 然後parse_args 但是也可以直接用Namespace去得到一個帶有這些參數的物件
# 然後你就可以使用args.ckptpath
args = argparse.Namespace(**args)

"""## Model definition and checkpoint loading"""

# 這是助教的示範 model，寫作業時一樣要換成自己的


class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)
        # input 維度 [3, 128, 128]
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, 1, 1),  # [64, 128, 128]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]

            nn.Conv2d(64, 128, 3, 1, 1),  # [128, 64, 64]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, 1, 1),  # [128, 64, 64]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),              # [128, 32, 32]

            nn.Conv2d(128, 256, 3, 1, 1),  # [256, 32, 32]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, 1, 1),  # [256, 32, 32]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [256, 16, 16]

            nn.Conv2d(256, 512, 3, 1, 1),  # [512, 16, 16]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.Conv2d(512, 512, 3, 1, 1),  # [512, 16, 16]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [512, 8, 8]

            nn.Conv2d(512, 512, 3, 1, 1),  # [512, 8, 8]
            nn.BatchNorm2d(512),
            nn.ReLU(),

            nn.Conv2d(512, 256, 3, 1, 1),  # [512, 8, 8]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]
        )
        self.fc = nn.Sequential(
            nn.Linear(256*4*4, 1024),
            nn.Dropout(p=0.5),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.Dropout(p=0.5),
            nn.ReLU(),
            nn.Linear(512, 11),

        )

    def forward(self, x):
        out = self.cnn(x)
        out = out.view(out.size()[0], -1)
        return self.fc(out)


model = Classifier().cuda()

# 其實可以直接在load_state_dict()中放torch.load
print("Loading the model...")
model.load_state_dict(torch.load(args.ckptpath))
print("Total parameters:{}".format(
    sum([p.numel() for p in model.parameters()])))
print("Trainable parameters:{}".format(sum([p.numel()
                                            for p in model.parameters() if p.requires_grad])))
# 基本上出現 <All keys matched successfully> 就是有載入成功，但最好還是做一下 inference 確認 test accuracy 沒有錯。

"""## Dataset definition and creation"""

# 助教 training 時定義的 dataset
# 因為 training 的時候助教有使用底下那些 transforms，所以 testing 時也要讓 test data 使用同樣的 transform
# dataset 這部分的 code 基本上不應該出現在你的作業裡，你應該使用自己當初 train HW3 時的 preprocessing

# training 時做 data augmentation
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    # 本作業因為是要做explainable AI 因此視同test_transform
    # transforms.RandomHorizontalFlip(),  # 隨機將圖片水平翻轉
    # transforms.RandomRotation(15),  # 隨機旋轉圖片
    # 將圖片轉成 Tensor，並把數值normalize到[0,1](data normalization)
    # totensor()把(128,128,3)轉換成tensor
    transforms.ToTensor(),
])
# testing 時不需做 data augmentation
test_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
])


class ImgDataset(Dataset):
    def __init__(self, x, y=None, transform=None):
        self.x = x
        # label is required to be a LongTensor
        self.y = y
        if y is not None:
            self.y = torch.LongTensor(y)
        self.transform = transform

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        X = self.x[index]
        # make a seed with numpy generator
        seed = np.random.randint(2147483647)
        random.seed(seed)

        if self.transform is not None:
            X = self.transform(X)
        if self.y is not None:
            Y = self.y[index]
            return X, Y
        else:
            return X
    #   # 這個 method 並不是 pytorch dataset 必要，只是方便未來我們想要指定「取哪幾張圖片」出來當作一個 batch 來 visualize

    def getbatch(self, indices):
        # 此function會回傳一連串index的x value 跟 ylabel
        # return x:(len(indices),3,128,128)) y:(len(indices))
        images = []
        labels = []
        for index in indices:
            image, label = self.__getitem__(index)

            images.append(image)
            labels.append(label)
        return torch.stack(images), torch.tensor(labels)
# 給予 data 的路徑，回傳每一張圖片的「路徑」和「class」


def readfile(path, label):
    # label 是一個 boolean variable，代表需不需要回傳 y 值
    # 此function回傳image的 value x 跟label y
    image_dir = sorted(os.listdir(path))
    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)
    y = np.zeros((len(image_dir)), dtype=np.uint8)
    for i, file in enumerate(image_dir):
        img = cv2.imread(os.path.join(path, file))
        # cv2可以讀取圖片資料 (width, height, channel)
        x[i, :, :] = cv2.resize(img, (128, 128))
        if label:
            y[i] = int(file.split("_")[0])
    if label:
        return x, y
    else:
        return x


train_x, train_y = readfile(os.path.join(args.dataset_dir, "training"), True)
val_x, val_y = readfile(os.path.join(args.dataset_dir, "validation"), True)

# train_x (data數量,128,128,3)
print("Size of Training data = {}".format(len(train_x)))
print("Size of validation data = {}".format(len(val_x)))
# 這邊在 initialize dataset 時只丟「路徑」和「class」，之後要從 dataset 取資料時
# dataset 的 __getitem__ method 才會動態的去 load 每個路徑對應的圖片
train_set = ImgDataset(train_x, train_y, transform=train_transform)
val_set = ImgDataset(val_x, transform=test_transform)
val_loader = DataLoader(val_set, batch_size=64, shuffle=False)


"""## Start Homework 4

# Saliency map

# 我們把一張圖片丟進 model，forward 後與 label 計算出 loss。
# 因此與 loss 相關的有:
# - image
# - model parameter
# - label

# 通常的情況下，我們想要改變 model parameter 來 fit image 和 label。因此 loss 在計算 backward 時我們只在乎 **loss 對 model parameter** 的偏微分值。但數學上 image 本身也是 continuous tensor，我們可以計算  **loss 對 image** 的偏微分值。這個偏微分值代表「在 model parameter 和 label 都固定下，稍微改變 image 的某個 pixel value 會對 loss 產生什麼變化」。人們習慣把這個變化的劇烈程度解讀成該 pixel 的重要性 (每個 pixel 都有自己的偏微分值)。因此把同一張圖中，loss 對每個 pixel 的偏微分值畫出來，就可以看出該圖中哪些位置是 model 在判斷時的重要依據。

# 實作上非常簡單，過去我們都是 forward 後算出 loss，然後進行 backward。而這個 backward，pytorch 預設是計算 **loss 對 model parameter** 的偏微分值，因此我們只需要用一行 code 額外告知 pytorch，**image** 也是要算偏微分的對象之一。
# """


def normalize(image):
    # image:(3,128,128)

    return (image - image.min()) / (image.max() - image.min())


def compute_saliency_maps(x, y, model):
    # model變成evaluation就不會做gradient
    # x變成require gradient就會計算gradient
    model.eval()
    x = x.cuda()

    # 最關鍵的一行 code
    # 因為我們要計算 loss 對 input image 的微分，原本 input x 只是一個 tensor，預設不需要 gradient
    # 這邊我們明確的告知 pytorch 這個 input x 需要gradient，這樣我們執行 backward 後 x.grad 才會有微分的值
    x.requires_grad_()

    y_pred = model(x)
    loss_func = torch.nn.CrossEntropyLoss()
    loss = loss_func(y_pred, y.cuda())
    # loss.backward()計算gradient optimizer step才是更新x
    loss.backward()

    saliencies = x.grad.abs().detach().cpu()
    # saliencies: (batches, channels, height, weight)
    # 因為接下來我們要對每張圖片畫 saliency map，每張圖片的 gradient scale 很可能有巨大落差
    # 可能第一張圖片的 gradient 在 100 ~ 1000，但第二張圖片的 gradient 在 0.001 ~ 0.0001
    # 如果我們用同樣的色階去畫每一張 saliency 的話，第一張可能就全部都很亮，第二張就全部都很暗，
    # 如此就看不到有意義的結果，我們想看的是「單一張 saliency 內部的大小關係」，
    # 所以這邊我們要對每張 saliency 各自做 normalize。手法有很多種，這邊只採用最簡單的
    saliencies = torch.stack([normalize(item) for item in saliencies])
    return saliencies


if not os.path.isdir(os.path.join(sys.argv[2], 'Question1')):
    os.makedirs(os.path.join(sys.argv[2], 'Question1'))
for i in range(1, 12):
    # 指定想要一起 visualize 的圖片 indices
    if i == 1:
        img_indices = [0, 1, 2]
    if i == 2:
        img_indices = [994, 995, 996]
    if i == 3:
        img_indices = [1423, 1424, 1425]
    if i == 4:
        img_indices = [2923, 2924, 2925]
    if i == 5:
        img_indices = [3909, 3910, 3911]
    if i == 6:
        img_indices = [4757, 4758, 4759]
    if i == 7:
        img_indices = [6082, 6083, 6084]
    if i == 8:
        img_indices = [6522, 6523, 6524]
    if i == 9:
        img_indices = [6802, 6803, 6804]
    if i == 10:
        img_indices = [7657, 7658, 7659]
    if i == 11:
        img_indices = [9157, 9158, 9159]
    images, labels = train_set.getbatch(img_indices)
    saliencies = compute_saliency_maps(images, labels, model)
    # saliencies(batches, channels, height, weight)
    # 使用 matplotlib 畫出來 fig是整個畫布 axs是每一張子圖
    fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
    for row, target in enumerate([images, saliencies]):
        for column, img in enumerate(target):
            # row column代表作圖坐在哪個位置 每一個target會是images中其中一個image或是saliency map中其中一個image
            # 圖片前面用cv2處理 所以模式會是bgr 但是plt.imshow是RGB所以要轉換

            b, g, r = cv2.split(img.permute(1, 2, 0).numpy())
            img = cv2.merge([r, g, b])
            axs[row][column].imshow(img)
            # 小知識：permute 是什麼，為什麼這邊要用?
            # 在 pytorch 的世界，image tensor 各 dimension 的意義通常為 (channels, height, width)
            # 但在 matplolib 的世界，想要把一個 tensor 畫出來，形狀必須為 (height, width, channels)
            # 因此 permute 是一個 pytorch 很方便的工具來做 dimension 間的轉換
            # 這邊 img.permute(1, 2, 0)，代表轉換後的 tensor，其
            # - 第 0 個 dimension 為原本 img 的第 1 個 dimension，也就是 height
            # - 第 1 個 dimension 為原本 img 的第 2 個 dimension，也就是 width
            # - 第 2 個 dimension 為原本 img 的第 0 個 dimension，也就是 channels

    plt.savefig(os.path.join(
        sys.argv[2], 'Question1', 'SaliencyMap'+str(i)+'.png'))
    plt.close()


# 從第二張圖片的 saliency，我們可以發現 model 有認出蛋黃的位置
# 從第三、四張圖片的 saliency，雖然不知道 model 細部用食物的哪個位置判斷，但可以發現 model 找出了食物的大致輪廓

"""## Filter explaination

這裡我們想要知道某一個 filter 到底認出了什麼。我們會做以下兩件事情：
- Filter activation: 挑幾張圖片出來，看看圖片中哪些位置會 activate 該 filter
- Filter visualization: 怎樣的 image 可以最大程度的 activate 該 filter

實作上比較困難的地方是，通常我們是直接把 image 丟進 model，一路 forward 到底。如：
```
loss = model(image)
loss.backward()
```
我們要怎麼得到中間某層 CNN 的 output? 當然我們可以直接修改 model definition，讓 forward 不只 return loss，也 return activation map。但這樣的寫法麻煩了，更改了 forward 的 output 可能會讓其他部分的 code 要跟著改動。因此 pytorch 提供了方便的 solution: **hook**，以下我們會再介紹。
"""


def normalize(image):
    return (image - image.min()) / (image.max() - image.min())


layer_activations = None


def filter_explaination(x, model, cnnid, filterid, iteration=100, lr=1):
    # x: 要用來觀察哪些位置可以 activate 被指定 filter 的圖片們
    # cnnid, filterid: 想要指定第幾層 cnn 中第幾個 filter
    model.eval()

    def hook(model, input, output):
        global layer_activations
        # 通過某一層cnn後 他就會呼叫hook這個function 然後幫我們把output存在layer_activation這個變數
        layer_activations = output

    hook_handle = model.cnn[cnnid].register_forward_hook(hook)
    # 這一行是在告訴 pytorch，當 forward 「過了」第 cnnid 層 cnn 後，要先呼叫 hook 這個我們定義的 function 後才可以繼續 forward 下一層 cnn
    # 因此上面的 hook function 中，我們就會把該層的 output，也就是 activation map 記錄下來，這樣 forward 完整個 model 後我們就不只有 loss
    # 也有某層 cnn 的 activation map
    # 注意：到這行為止，都還沒有發生任何 forward。我們只是先告訴 pytorch 等下真的要 forward 時該多做什麼事
    # 注意：hook_handle 可以先跳過不用懂，等下看到後面就有說明了

    # Filter activation: 我們先觀察 x 經過被指定 filter 的 activation map
    model(x.cuda())
    # 這行才是正式執行 forward，因為我們只在意 activation map，所以這邊不需要把 loss 存起來
    # layer activations的維度也跟conv2d一樣(batch, filter, width, height)
    # 若我們只想要特定filter的結果就可以這樣寫
    filter_activations = layer_activations[:, filterid, :, :].detach().cpu()

    # 根據 function argument 指定的 filterid 把特定 filter 的 activation map 取出來
    # 因為目前這個 activation map 我們只是要把他畫出來，所以可以直接 detach from graph 並存成 cpu tensor

    # Filter visualization: 接著我們要找出可以最大程度 activate 該 filter 的圖片
    x = x.cuda()
    # 從一張 random noise 的圖片開始找 (也可以從一張 dataset image 開始找)
    x.requires_grad_()
    # 我們要對 input image 算偏微分
    optimizer = Adam([x], lr=lr)
    # 利用偏微分和 optimizer，逐步修改 input image 來讓 filter activation 越來越大
    for iter in range(iteration):
        optimizer.zero_grad()
        model(x)

        objective = -layer_activations[:, filterid, :, :].sum()
        # 與上一個作業不同的是，我們並不想知道 image 的微量變化會怎樣影響 final loss
        # 我們想知道的是，image 的微量變化會怎樣影響 activation 的程度
        # 因此 objective 是 filter activation 的加總，然後加負號代表我們想要做 maximization
        # 可以把objective想成loss function 他做微分就是去找x對他的微分 然後step讓x改變使loss越小越好 這樣就會讓activation變大(加入負號)
        objective.backward()
        # 計算 filter activation 對 input image 的偏微分
        optimizer.step()
        # 修改 input image 來最大化 filter activation
    filter_visualization = x.detach().cpu().squeeze()[:]
    # 完成圖片修改，只剩下要畫出來，因此可以直接 detach 並轉成 cpu tensor

    hook_handle.remove()
    # 很重要：一旦對 model register hook，該 hook 就一直存在。如果之後繼續 register 更多 hook
    # 那 model 一次 forward 要做的事情就越來越多，甚至其行為模式會超出你預期 (因為你忘記哪邊有用不到的 hook 了)
    # 因此事情做完了之後，就把這個 hook 拿掉，下次想要再做事時再 register 就好了。

    return filter_activations, filter_visualization


if not os.path.isdir(os.path.join(sys.argv[2], 'Question2')):
    os.makedirs(os.path.join(sys.argv[2], 'Question2'))
for j in range(1, 4):
    if j == 1:
        img_indices = [0, 994, 1423, 2923]
    if j == 2:
        img_indices = [3909, 4757, 6082]
    if j == 3:
        img_indices = [6522, 6802, 7657, 9157]

    images, labels = train_set.getbatch(img_indices)
    filter_activations, filter_visualization = filter_explaination(
        images, model, cnnid=15, filterid=0, iteration=100, lr=0.1)

    # 畫出 filter visualization(channel,width ,height)轉換成(width, height, channel)
    for k in range(len(img_indices)):
        plt.subplot(1, 4, k+1)
        b, g, r = cv2.split(filter_visualization[k].permute(1, 2, 0).numpy())
        visualization = cv2.merge([r, g, b])
        plt.imshow(normalize(visualization))
    plt.savefig(os.path.join(
        sys.argv[2], 'Question2', 'Filter_visualization_x'+str(j)+'.png'))
    plt.close()
    # 根據圖片中的線條，可以猜測第 15 層 cnn 其第 0 個 filter 可能在認一些線條、甚至是 object boundary
    # 因此給 filter 看一堆對比強烈的線條，他會覺得有好多 boundary 可以 activate

    # 畫出 filter activations
    fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
    for i, img in enumerate(images):
        b, g, r = cv2.split(img.permute(1, 2, 0).numpy())
        img = cv2.merge([r, g, b])

        axs[0][i].imshow(img)
    for i, img in enumerate(filter_activations):

        axs[1][i].imshow(normalize(img))
    plt.savefig(os.path.join(
        sys.argv[2], 'Question2', 'Filter'+str(0+1)+'_activation'+str(j)+'.png'))
    plt.close()

# 第二個filter
img_indices = [0, 994, 1423, 2923]


images, labels = train_set.getbatch(img_indices)
filter_activations, filter_visualization = filter_explaination(
    images, model, cnnid=1, filterid=0, iteration=100, lr=0.1)

# 畫出 filter visualization(channel,width ,height)轉換成(width, height, channel)
for k in range(len(img_indices)):
    plt.subplot(1, 4, k+1)
    b, g, r = cv2.split(filter_visualization[k].permute(1, 2, 0).numpy())
    visualization = cv2.merge([r, g, b])
    plt.imshow(normalize(visualization))
plt.savefig(os.path.join(
    sys.argv[2], 'Question2', 'CNN'+str(1)+'Filter_visualization_x.png'))
plt.close()
# 根據圖片中的線條，可以猜測第 15 層 cnn 其第 0 個 filter 可能在認一些線條、甚至是 object boundary
# 因此給 filter 看一堆對比強烈的線條，他會覺得有好多 boundary 可以 activate

# 畫出 filter activations
fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
for i, img in enumerate(images):
    b, g, r = cv2.split(img.permute(1, 2, 0).numpy())
    img = cv2.merge([r, g, b])

    axs[0][i].imshow(img)
for i, img in enumerate(filter_activations):

    axs[1][i].imshow(normalize(img))
plt.savefig(os.path.join(
    sys.argv[2], 'Question2', 'CNN'+str(1)+'Filter'+str(0+1)+'_activation.png'))
plt.close()
# 第三個filter
img_indices = [0, 994, 1423, 2923]


images, labels = train_set.getbatch(img_indices)
filter_activations, filter_visualization = filter_explaination(
    images, model, cnnid=4, filterid=10, iteration=100, lr=0.1)

# 畫出 filter visualization(channel,width ,height)轉換成(width, height, channel)
for k in range(len(img_indices)):
    plt.subplot(1, 5, k+1)
    b, g, r = cv2.split(filter_visualization[k].permute(1, 2, 0).numpy())
    visualization = cv2.merge([r, g, b])
    plt.imshow(normalize(visualization))
plt.savefig(os.path.join(
    sys.argv[2], 'Question2', 'CNN'+str(5)+'Filter_visualization_x.png'))
plt.close()
# 根據圖片中的線條，可以猜測第 15 層 cnn 其第 0 個 filter 可能在認一些線條、甚至是 object boundary
# 因此給 filter 看一堆對比強烈的線條，他會覺得有好多 boundary 可以 activate

# 畫出 filter activations
fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
for i, img in enumerate(images):
    b, g, r = cv2.split(img.permute(1, 2, 0).numpy())
    img = cv2.merge([r, g, b])

    axs[0][i].imshow(img)
for i, img in enumerate(filter_activations):

    axs[1][i].imshow(normalize(img))
plt.savefig(os.path.join(
    sys.argv[2], 'Question2', 'CNN'+str(5)+'Filter'+str(10)+'_activation.png'))
plt.close()

# 從下面四張圖可以看到，activate 的區域對應到一些物品的邊界，尤其是顏色對比較深的邊界

"""## Lime

Lime 的部分因為有現成的套件可以使用，因此下方直接 demo 如何使用該套件。其實非常的簡單，只需要 implement 兩個 function 即可。
"""


def predict(input):
    # input: numpy array, (batches, height, width, channels)

    model.eval()
    input = torch.FloatTensor(input).permute(0, 3, 1, 2)
    # 需要先將 input 轉成 pytorch tensor，且符合 pytorch 習慣的 dimension 定義
    # 也就是 (batches, channels, height, width)

    output = model(input.cuda())
    return output.detach().cpu().numpy()


def segmentation(input):
    # 利用 skimage 提供的 segmentation 將圖片分成 100 塊
    return slic(input, n_segments=100, compactness=1, sigma=1)


def explain(explainer, image, classifier_fn, segmentation_fn):
    np.random.seed(12)
    return explainer.explain_instance(
        image=image, classifier_fn=classifier_fn, segmentation_fn=segmentation_fn)


for i in range(3):
    if i == 0:
        img_indices = [0, 1, 2]

    if i == 1:
        img_indices = [1423, 1424, 1425]

    if i == 2:
        img_indices = [1702, 1703, 1704]
    images, labels = train_set.getbatch(img_indices)
    fig, axs = plt.subplots(2, 3, figsize=(15, 8))
    np.random.seed(16)
    # 讓實驗 reproducible
    for idx, (image, label) in enumerate(zip(images.permute(0, 2, 3, 1).numpy(), labels)):
        x = image.astype(np.double)
        # lime 這個套件要吃 numpy array
        # 看來lime要吃的input維度是(batch, height, width, channel)
        explainer = lime_image.LimeImageExplainer()
        explaination = explain(explainer,
                               image=x, classifier_fn=predict, segmentation_fn=segmentation)
        # 基本上只要提供給 lime explainer 兩個關鍵的 function，事情就結束了
        # classifier_fn 定義圖片如何經過 model 得到 prediction
        # segmentation_fn 定義如何把圖片做 segmentation
        # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explain_instance#lime.lime_image.LimeImageExplainer.explain_instance

        lime_img, mask = explaination.get_image_and_mask(
            label=label.item(),
            positive_only=False,
            hide_rest=False,
            num_features=11,
            min_weight=0.05
        )
        # 把 explainer 解釋的結果轉成圖片
        # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=get_image_and_mask#lime.lime_image.ImageExplanation.get_image_and_mask
        b, g, r = cv2.split(image)
        image = cv2.merge([r, g, b])
        b, g, r = cv2.split(lime_img)
        lime_img = cv2.merge([r, g, b])
        axs[0][idx].imshow(image)
        axs[1][idx].imshow(lime_img)
    if not os.path.isdir(os.path.join(sys.argv[2], 'Lime_question3')):
        os.makedirs(os.path.join(sys.argv[2], 'Lime_question3'))
    plt.savefig(os.path.join(
        sys.argv[2], 'Lime_question3', 'Lime'+str(i)+'.png'))
    plt.close()
# 從以下前三章圖可以看到，model 有認出食物的位置，並以該位置為主要的判斷依據
# 唯一例外是第四張圖，看起來 model 似乎比較喜歡直接去認「碗」的形狀，來判斷該圖中屬於 soup 這個 class
# 至於碗中的內容物被標成紅色，代表「單看碗中」的東西反而有礙辨認。
# 當 model 只看碗中黃色的一坨圓形，而沒看到「碗」時，可能就會覺得是其他黃色圓形的食物。

# confusion matrix plot
val_predict = []
with torch.no_grad():
    for i, data in enumerate(val_loader):

        test_pred = model(data.cuda())
        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)
        for y in test_label:
            val_predict.append(y)


confusion_matrix = np.zeros([11, 11],  dtype=np.float)
for i in range(len(val_y)):
    confusion_matrix[val_y[i], val_predict[i]] += 1
for i in range(len(confusion_matrix)):
    confusion_matrix[i] /= np.sum(confusion_matrix[i], axis=0)
# 然後用plt.matshow()
plt.matshow(confusion_matrix)
ind_array = np.arange(0, len(confusion_matrix), 1)
# x,y各自代表這張圖的座標位置 我們想在(0,0)(0,1)....(n,n)的grid中間填入數字
x, y = np.meshgrid(ind_array, ind_array)

# 用 plt.text加字
for x_val, y_val in zip(x.flatten(), y.flatten()):
    c = "{:.2f}".format(confusion_matrix[y_val][x_val])
    # 經過va='center'之後就會跑到中間
    plt.text(x_val, y_val, c, va='center', ha='center', fontsize=8)

# 設置每一個grid的label名稱 rotation就是把字體稍微逆時針旋轉
plt.xticks([i for i in range(len(confusion_matrix))],
           labels=['Bread', 'Dairy', 'Dessert', 'Egg', 'Fried food', 'Meat', 'Noodles/Pasta', 'Rice',
                   'Seafood', 'Soup', 'Vege/Fruit'], fontsize=8, rotation=60, color='blue')
plt.yticks([i for i in range(len(confusion_matrix))],
           labels=['Bread', 'Dairy', 'Dessert', 'Egg', 'Fried food', 'Meat', 'Noodles/Pasta', 'Rice',
                   'Seafood', 'Soup', 'Vege/Fruit'], fontsize=8, color='red')
plt.colorbar()
plt.savefig(os.path.join(
    sys.argv[2], 'Confusion_matrix.png'))
plt.close()


# Question4 Smooth grad


def smooth_grad(x, y, model, sigma=1):
    # model變成evaluation就不會做gradient
    # x變成require gradient就會計算gradient
    model.eval()
    x = x.cuda()
    torch.manual_seed(0)

    # 決定Noise的數值(基本上就是gaussian distribution N(0,sigma**2))
    # 因為randn是N(0,1) 要把noise再來乘以sigma才能得到N(0,simga**2)
    # x本身已經被normalize到(0,1)之間了 所以要想辦法把simga=1對準到 std(x)
    sigma = sigma * (torch.max(x)-torch.min(x)).item()
    noise = torch.randn(10)
    noise *= sigma
    # 最關鍵的一行 code
    # 因為我們要計算 loss 對 input image 的微分，原本 input x 只是一個 tensor，預設不需要 gradient
    # 這邊我們明確的告知 pytorch 這個 input x 需要gradient，這樣我們執行 backward 後 x.grad 才會有微分的值
    noise_x = x + noise[0]
    noise_x.requires_grad_()
    y_pred = model(noise_x)
    loss_func = torch.nn.CrossEntropyLoss()
    loss = loss_func(y_pred, y.cuda())
    # loss.backward()計算gradient optimizer step才是更新x
    loss.backward()

    saliencies = noise_x.grad.abs().detach().cpu()

    for i in range(1, 10):
        noise_x = x + noise[i]
        noise_x.requires_grad_()
        y_pred = model(noise_x)
        loss_func = torch.nn.CrossEntropyLoss()
        loss = loss_func(y_pred, y.cuda())
        # loss.backward()計算gradient optimizer step才是更新x
        loss.backward()

        saliencies += noise_x.grad.abs().detach().cpu()
    saliencies /= 10
    # saliencies: (batches, channels, height, weight)
    # 因為接下來我們要對每張圖片畫 saliency map，每張圖片的 gradient scale 很可能有巨大落差
    # 可能第一張圖片的 gradient 在 100 ~ 1000，但第二張圖片的 gradient 在 0.001 ~ 0.0001
    # 如果我們用同樣的色階去畫每一張 saliency 的話，第一張可能就全部都很亮，第二張就全部都很暗，
    # 如此就看不到有意義的結果，我們想看的是「單一張 saliency 內部的大小關係」，
    # 所以這邊我們要對每張 saliency 各自做 normalize。手法有很多種，這邊只採用最簡單的
    saliencies = torch.stack([normalize(item) for item in saliencies])
    return saliencies


def smooth_grad2(x, y, model, sigma=1):
    # model變成evaluation就不會做gradient
    # x變成require gradient就會計算gradient
    model.eval()
    x = x.cuda()
    torch.manual_seed(0)

    # 決定Noise的數值(基本上就是gaussian distribution N(0,sigma**2))
    # 因為randn是N(0,1) 要把noise再來乘以sigma才能得到N(0,simga**2)
    # x本身已經被normalize到(0,1)之間了 所以要想辦法把simga=1對準到 std(x)
    sigma = sigma * (torch.max(x)-torch.min(x)).item()
    noise = torch.randn(50)
    noise *= sigma
    # 最關鍵的一行 code
    # 因為我們要計算 loss 對 input image 的微分，原本 input x 只是一個 tensor，預設不需要 gradient
    # 這邊我們明確的告知 pytorch 這個 input x 需要gradient，這樣我們執行 backward 後 x.grad 才會有微分的值
    noise_x = x + noise[0]
    noise_x.requires_grad_()
    y_pred = model(noise_x)
    loss_func = torch.nn.CrossEntropyLoss()
    loss = loss_func(y_pred, y.cuda())
    # loss.backward()計算gradient optimizer step才是更新x
    loss.backward()

    saliencies = noise_x.grad.abs().detach().cpu()

    for i in range(1, 50):
        noise_x = x + noise[i]
        noise_x.requires_grad_()
        y_pred = model(noise_x)
        loss_func = torch.nn.CrossEntropyLoss()
        loss = loss_func(y_pred, y.cuda())
        # loss.backward()計算gradient optimizer step才是更新x
        loss.backward()

        saliencies += noise_x.grad.abs().detach().cpu()
    saliencies /= 50
    # saliencies: (batches, channels, height, weight)
    # 因為接下來我們要對每張圖片畫 saliency map，每張圖片的 gradient scale 很可能有巨大落差
    # 可能第一張圖片的 gradient 在 100 ~ 1000，但第二張圖片的 gradient 在 0.001 ~ 0.0001
    # 如果我們用同樣的色階去畫每一張 saliency 的話，第一張可能就全部都很亮，第二張就全部都很暗，
    # 如此就看不到有意義的結果，我們想看的是「單一張 saliency 內部的大小關係」，
    # 所以這邊我們要對每張 saliency 各自做 normalize。手法有很多種，這邊只採用最簡單的
    saliencies = torch.stack([normalize(item) for item in saliencies])
    return saliencies


if not os.path.isdir(os.path.join(sys.argv[2], 'Question4')):
    os.makedirs(os.path.join(sys.argv[2], 'Question4'))
for i in range(1, 6):
    # 指定想要一起 visualize 的圖片 indices

    img_indices = [0, 1, 2]

    images, labels = train_set.getbatch(img_indices)
    if i != 5:
        saliencies = smooth_grad(images, labels, model, sigma=i)
    else:
        saliencies = smooth_grad2(images, labels, model, sigma=3/4)
    # saliencies(batches, channels, height, weight)
    # 使用 matplotlib 畫出來 fig是整個畫布 axs是每一張子圖
    fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
    for row, target in enumerate([images, saliencies]):
        for column, img in enumerate(target):
            # row column代表作圖坐在哪個位置 每一個target會是images中其中一個image或是saliency map中其中一個image
            # 圖片前面用cv2處理 所以模式會是bgr 但是plt.imshow是RGB所以要轉換

            b, g, r = cv2.split(img.permute(1, 2, 0).numpy())
            img = cv2.merge([r, g, b])
            axs[row][column].imshow(img)
            # 小知識：permute 是什麼，為什麼這邊要用?
            # 在 pytorch 的世界，image tensor 各 dimension 的意義通常為 (channels, height, width)
            # 但在 matplolib 的世界，想要把一個 tensor 畫出來，形狀必須為 (height, width, channels)
            # 因此 permute 是一個 pytorch 很方便的工具來做 dimension 間的轉換
            # 這邊 img.permute(1, 2, 0)，代表轉換後的 tensor，其
            # - 第 0 個 dimension 為原本 img 的第 1 個 dimension，也就是 height
            # - 第 1 個 dimension 為原本 img 的第 2 個 dimension，也就是 width
            # - 第 2 個 dimension 為原本 img 的第 0 個 dimension，也就是 channels

    plt.savefig(os.path.join(
        sys.argv[2], 'Question4', 'Smooth_grad_SIGMA'+str(i)+'.png'))
    plt.close()

# Question4 Gradient Times image


def convert_to_grayscale(im_as_arr):
    """
        Converts 3d image to grayscale
    Args:
        im_as_arr (numpy arr): RGB image with shape (D,W,H)
    returns:
        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)
    """
    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)
    im_max = np.percentile(grayscale_im, 99)
    im_min = np.min(grayscale_im)
    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))
    grayscale_im = np.expand_dims(grayscale_im, axis=0)
    return grayscale_im


if not os.path.isdir(os.path.join(sys.argv[2], 'Question4')):
    os.makedirs(os.path.join(sys.argv[2], 'Question4'))
for i in range(1, 2):
    # 指定想要一起 visualize 的圖片 indices

    img_indices = [0, 1, 2]

    images, labels = train_set.getbatch(img_indices)

    saliencies = compute_saliency_maps(images, labels, model)
    saliencies = normalize(images * saliencies*100000)
    # saliencies(batches, channels, height, weight)
    # 使用 matplotlib 畫出來 fig是整個畫布 axs是每一張子圖
    fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))
    for row, target in enumerate([images, saliencies]):
        for column, img in enumerate(target):
            # row column代表作圖坐在哪個位置 每一個target會是images中其中一個image或是saliency map中其中一個image
            # 圖片前面用cv2處理 所以模式會是bgr 但是plt.imshow是RGB所以要轉換
            if row == 0:
                b, g, r = cv2.split(img.permute(1, 2, 0).numpy())
                img = cv2.merge([r, g, b])
                axs[row][column].imshow(img)
            else:
                img = convert_to_grayscale(img.numpy())
                img = torch.tensor(img).permute(
                    1, 2, 0).view([128, 128]).numpy()
                axs[row][column].imshow(img, cmap='gray')
            # 小知識：permute 是什麼，為什麼這邊要用?
            # 在 pytorch 的世界，image tensor 各 dimension 的意義通常為 (channels, height, width)
            # 但在 matplolib 的世界，想要把一個 tensor 畫出來，形狀必須為 (height, width, channels)
            # 因此 permute 是一個 pytorch 很方便的工具來做 dimension 間的轉換
            # 這邊 img.permute(1, 2, 0)，代表轉換後的 tensor，其
            # - 第 0 個 dimension 為原本 img 的第 1 個 dimension，也就是 height
            # - 第 1 個 dimension 為原本 img 的第 2 個 dimension，也就是 width
            # - 第 2 個 dimension 為原本 img 的第 0 個 dimension，也就是 channels

    plt.savefig(os.path.join(
        sys.argv[2], 'Question4', 'GRADTimesImage.png'))
    plt.close()
